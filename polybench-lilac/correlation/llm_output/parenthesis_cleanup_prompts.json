{
  "system_prompt": "You are an expert C programmer specializing in code readability and style.\nYour task is to analyze C code and identify unnecessary parentheses that can be safely removed without changing program behavior or readability.\n\nConsider these guidelines when deciding which parentheses to remove:\n- Remove parentheses around single variables: `(x)` \u2192 `x`\n- Remove redundant parentheses that don't affect operator precedence\n- Keep parentheses that clarify complex expressions or override precedence\n- Keep parentheses in function calls, declarations, and control structures\n- Consider readability: sometimes parentheses help clarity even if technically unnecessary\n\nFor each unnecessary parenthesis you identify, suggest:\n1. The exact line number where it appears\n2. The parenthesis expression to remove\n3. Why it can be safely removed\n\nReturn your response as JSON with this structure:\n{\n    \"removals\": [\n        {\n            \"line_number\": 42,\n            \"expression\": \"(x + y)\",\n            \"replacement\": \"x + y\",\n            \"reasoning\": \"Parentheses are unnecessary as + has lower precedence than surrounding operators\"\n        }\n    ]\n}\n\nOnly suggest removals for parentheses that are truly unnecessary and improve readability.",
  "user_prompt": "Analyze this C code and identify unnecessary parentheses that can be safely removed.\n\nFound 138 potential parentheses to evaluate:\n\nLine 33: struct Dim3Coerce;\n  Potential parentheses to evaluate: (__GNUC__)\nLine 35: /* Function definitions */\n\n/* Types Definitions */\n  Potential parentheses to evaluate: (x)\nLine 85: /* External Global Variable Declarations */\n  Potential parentheses to evaluate: (X)\nLine 271: // INSERT COMMENT IFELSE: kernel_diag::entry\n  Potential parentheses to evaluate: (noinline)\nLine 482: \n  Potential parentheses to evaluate: (noinline), (uint32_t, uint32_t), (noinline, nothrow), (double), (double), (noinline, nothrow), (noinline, nothrow), (noinline, nothrow), (noinline, nothrow), (noinline, nothrow), (noinline, nothrow), (uint32_t a, uint32_t b), (uint64_t a, uint64_t b), (uint32_t a, uint32_t b), (uint32_t a, uint32_t b), (uint64_t a, uint64_t b), (int32_t a, int32_t b), (int32_t a, int32_t b), (argv[1]), (argv[2]), (argv[3]), (double*), (double*), (double*), (double*), (double*), (double*), (uint8_t*), (double*), (uint8_t*), (double*), (uint8_t*), (double*), (uint8_t*), (double*), (double), (i), (double), (j), (fmt_double_space), (int), (int), (fmt_newline), (fmt_newline), (uint32_t num, uint32_t factor), (double), (n), (double), (n), (stddev[j]), (stddev[j], 0.10000000000000001), (double), (n), (m, 256), (uint8_t*), (&grid_mean_blocks_coerce), (uint8_t*), (&grid_mean_blocks), (uint8_t*), (&block_256x1_coerce), (uint8_t*), (&block_256x1), (2), (m, n, data, corr, mean, stddev, num_blocks_m_256, 1, 1, 256, 1, 1, i, 0, 0, j, 0, 0), (m, 256), (uint8_t*), (&grid_stddev_blocks_coerce), (uint8_t*), (&grid_stddev_blocks), (uint8_t*), (&block_256x1_stddev_coerce), (uint8_t*), (&block_256x1_stddev), (2), (m, n, data, corr, mean, stddev, num_blocks_m_256_2, 1, 1, 256, 1, 1, i, 0, 0, j, 0, 0), (n, block.batch), (m, block.x), (uint8_t*), (&block_8_32), (uint8_t*), (&block), (uint8_t*), (&grid_for_reduce), (uint8_t*), (&grid), (uint8_t*), (&block_8_32_coerce), (uint8_t*), (&block_8_32), (uint8_t*), (&grid_for_reduce_coerce), (uint8_t*), (&grid_for_reduce), (2), (m, n, data, corr, mean, stddev, 8, 32, 1, num_blocks_n_batch8, num_blocks_m_x32, 1, i, j, 0, k, l, 0), (m, 256), (uint8_t*), (&block_256x1_diag_coerce), (uint8_t*), (&block_256x1_diag), (uint8_t*), (&grid_diag_blocks_coerce), (uint8_t*), (&grid_diag_blocks), (2), (m, n, data, corr, mean, stddev, 256, 1, 1, num_blocks_m_256_diag, 1, 1, i, 0, 0, j, 0, 0), (uint8_t*), (&block_8_32_corr), (uint8_t*), (&block), (uint8_t*), (&grid_for_corr), (uint8_t*), (&grid), (uint8_t*), (&block_8_32_corr_coerce), (uint8_t*), (&block_8_32_corr), (uint8_t*), (&grid_for_corr_coerce), (uint8_t*), (&grid_for_corr), (2), (m, n, data, corr, mean, stddev, 8, 32, 1, num_blocks_m1_batch8, num_blocks_m1_x32, 1, i, j, 0, k, l, 0), (uint8_t*), (&block_tail_coerce), (uint8_t*), (&block_tail), (uint8_t*), (&grid_tail_coerce), (uint8_t*), (&grid_tail), (0), (m, n, data, corr, mean, stddev, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0)\n\nFor each unnecessary parenthesis, provide the exact expression to replace and explain why it can be safely removed. Focus only on parentheses that are truly redundant and don't affect readability.\n\nFull source code:\n```c\n/* Provide Declarations */\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n#ifndef __cplusplus\ntypedef unsigned char bool;\n#endif\n\n#ifndef _MSC_VER\n#define __forceinline __attribute__((always_inline)) inline\n#endif\n\n#if defined(__GNUC__)\n#define  __ATTRIBUTELIST__(x) __attribute__(x)\n#else\n#define  __ATTRIBUTELIST__(x)  \n#endif\n\n#ifdef _MSC_VER  /* Can only support \"linkonce\" vars with GCC */\n#define __attribute__(X)\n#endif\n\nstatic __forceinline int llvm_fcmp_ole(double X, double Y) { return X <= Y; }\n\n\n/* Global Declarations */\n\n/* Types Declarations */\nstruct IOFile;\nstruct Dim3;\nstruct Dim3Coerce;\n\n/* Function definitions */\n\n/* Types Definitions */\nstruct uint8_array_1 {\n  uint8_t array[1];\n};\nstruct uint8_array_20_t {\n  uint8_t array[20];\n};\nstruct IOFile {\n  uint32_t flags;\n  uint8_t* buf_base;\n  uint8_t* buf_ptr;\n  uint8_t* buf_end;\n  uint8_t* read_ptr;\n  uint8_t* read_end;\n  uint8_t* write_ptr;\n  uint8_t* write_end;\n  uint8_t* unget_buf;\n  uint8_t* line_buf;\n  uint8_t* cookie;\n  uint8_t* aux_buffer;\n  void* user_data;\n  struct IOFile* next;\n  uint32_t fd;\n  uint32_t status;\n  uint64_t position;\n  uint16_t orientation;\n  uint8_t charbuf;\n  uint8_t smallbuf[1];\n  uint8_t* locale;\n  uint64_t timestamp;\n  void* lock;\n  void* vtable;\n  struct IOFile* prev;\n  uint8_t* filename;\n  uint64_t reserved64;\n  uint32_t mode;\n  uint8_t padding[20];\n};\nstruct Dim3 {\n  uint32_t batch;\n  uint32_t x;\n  uint32_t y;\n};\nstruct Dim3Coerce {\n  uint64_t coerce_u64;\n  uint32_t coerce_u32;\n};\n\n/* External Global Variable Declarations */\n\n/* Function Declarations */\nuint32_t cudaSetupArgument(uint8_t*, uint64_t, uint64_t);\nuint32_t cudaLaunch(uint8_t*);\nint main(int, char **) __ATTRIBUTELIST__((noinline));\nvoid init_array(uint32_t, uint32_t, double*) __ATTRIBUTELIST__((noinline, nothrow));\nuint32_t cudaMemcpy(uint8_t*, uint8_t*, uint64_t, uint32_t);\nvoid kernel(uint32_t, uint32_t, double*, double*, double*, double*) __ATTRIBUTELIST__((noinline));\nvoid print_array(uint32_t, double*) __ATTRIBUTELIST__((noinline));\nuint32_t num_blocks(uint32_t, uint32_t) __ATTRIBUTELIST__((noinline, nothrow));\nuint32_t cudaConfigureCall(uint64_t, uint32_t, uint64_t, uint32_t, uint64_t, void*);\nuint32_t cudaMalloc(uint8_t**, uint64_t);\ndouble sqrt(double);\ndouble sqrt_OC_1(double);\nvoid kernel_mean(uint32_t, uint32_t, double*, double*, double*, double*, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t) __ATTRIBUTELIST__((noinline, nothrow));\nvoid kernel_stddev(uint32_t, uint32_t, double*, double*, double*, double*, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t) __ATTRIBUTELIST__((noinline, nothrow));\nvoid kernel_reduce(uint32_t, uint32_t, double*, double*, double*, double*, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t) __ATTRIBUTELIST__((noinline, nothrow));\nvoid kernel_diag(uint32_t, uint32_t, double*, double*, double*, double*, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t) __ATTRIBUTELIST__((noinline, nothrow));\nvoid kernel_corr(uint32_t, uint32_t, double*, double*, double*, double*, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t) __ATTRIBUTELIST__((noinline, nothrow));\nvoid kernel_tail(uint32_t, uint32_t, double*, double*, double*, double*, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t, uint32_t) __ATTRIBUTELIST__((noinline, nothrow));\n\n\n/* Global Variable Definitions and Initialization */\nuint8_t fmt_double_space[8] = { \"%0.2lf \" };\nuint8_t fmt_newline[2] = { \"\\n\" };\n\n\n/* LLVM Intrinsic Builtin Function Bodies */\nstatic __forceinline uint32_t llvm_add_u32(uint32_t a, uint32_t b) {\n  uint32_t r = a + b;\n  return r;\n}\nstatic __forceinline uint64_t llvm_add_u64(uint64_t a, uint64_t b) {\n  uint64_t r = a + b;\n  return r;\n}\nstatic __forceinline uint32_t llvm_sub_u32(uint32_t a, uint32_t b) {\n  uint32_t r = a - b;\n  return r;\n}\nstatic __forceinline uint32_t llvm_mul_u32(uint32_t a, uint32_t b) {\n  uint32_t r = a * b;\n  return r;\n}\nstatic __forceinline uint64_t llvm_mul_u64(uint64_t a, uint64_t b) {\n  uint64_t r = a * b;\n  return r;\n}\nstatic __forceinline uint32_t llvm_sdiv_u32(int32_t a, int32_t b) {\n  uint32_t r = a / b;\n  return r;\n}\nstatic __forceinline uint32_t llvm_srem_u32(int32_t a, int32_t b) {\n  uint32_t r = a % b;\n  return r;\n}\n\n\n/* Function Bodies */\nint main(int argc, char ** argv) {\n  int32_t dump_code;\n  int32_t n;\n  int32_t m;\n  uint8_t* data;\n  uint8_t* mean;\n  uint8_t* stddev;\n  uint8_t* corr;\n  int32_t placeholder_call34;\n  int32_t placeholder_call49;\n\n// INSERT COMMENT IFELSE: main::entry\n  dump_code = atoi(argv[1]);\n  n = atoi(argv[2]);\n  m = atoi(argv[3]);\n  data = malloc(n * m * 8);\n  mean = malloc(m * 8);\n  stddev = malloc(m * 8);\n  corr = malloc(m * m * 8);\n  init_array(m, n, ((double*)data));\n;\n  kernel(m, n, ((double*)data), ((double*)corr), ((double*)mean), ((double*)stddev));\n;\n  if (dump_code == 1) { // IFELSE MARKER: entry IF\nprint_array(m, ((double*)corr));\n  }\nfree(((uint8_t*)((double*)data)));\nfree(((uint8_t*)((double*)corr)));\nfree(((uint8_t*)((double*)mean)));\nfree(((uint8_t*)((double*)stddev)));\n  return 0;\n}\n// INSERT COMMENT FUNCTION: init_array\nvoid init_array(uint32_t m, uint32_t n, double* data) {\n  int64_t i;\n  int64_t j;\n\n// INSERT COMMENT LOOP: init_array::for.cond\nfor(int64_t i = 0; i < m;   i = i + 1){\nfor(int64_t j = 0; j < n;   j = j + 1){\n  data[(i * n + j)] = (((double)(i) * (double)(j)) / 1000);\n}\n}\n  return;\n}\n// INSERT COMMENT FUNCTION: print_array\nvoid print_array(uint32_t m, double* corr) {\n  int64_t i;\n  uint64_t j;\n  int32_t num_blocks_m_x32;\n\n// INSERT COMMENT LOOP: print_array::for.cond\nfor(int64_t i = 0; i < m;   i = i + 1){\nfor(int64_t j = 0; j < m;   j = j + 1){\n  fprintf(stderr, (fmt_double_space), corr[(i * m + j)]);\n  if ((int)(i * m + j) % (int)20 == 0) { // IFELSE MARKER: for.body3 IF\n  fprintf(stderr, (fmt_newline));\n  }\n}\n}\n  fprintf(stderr, (fmt_newline));\n}\n// INSERT COMMENT FUNCTION: num_blocks\nuint32_t num_blocks(uint32_t num, uint32_t factor) {\n  return ((num + factor) - 1) / factor;\n}\n// INSERT COMMENT FUNCTION: kernel_mean\nvoid kernel_mean(uint32_t m, uint32_t n, double* data, double* corr, double* mean, double* stddev, uint32_t gridDim_x, uint32_t gridDim_y, uint32_t gridDim_z, uint32_t blockDim_x, uint32_t blockDim_y, uint32_t blockDim_z, uint32_t blockIdx_x, uint32_t blockIdx_y, uint32_t blockIdx_z, uint32_t threadIdx_x, uint32_t threadIdx_y, uint32_t threadIdx_z) {\n  int64_t j;\n  int64_t i;\n\n// INSERT COMMENT IFELSE: kernel_mean::entry\n  j = blockDim_x * blockIdx_x + threadIdx_x;\n  if (j < m) { // IFELSE MARKER: entry IF\n  mean[j] = 0;\nfor(int64_t i = 0; i < n;   i = i + 1){\n  mean[j] = (mean[j] + data[(i * m + j)]);\n}\n  mean[j] = (mean[j] / (double)(n));\n  }\n  return;\n}\n// INSERT COMMENT FUNCTION: kernel_stddev\nvoid kernel_stddev(uint32_t m, uint32_t n, double* data, double* corr, double* mean, double* stddev, uint32_t gridDim_x, uint32_t gridDim_y, uint32_t gridDim_z, uint32_t blockDim_x, uint32_t blockDim_y, uint32_t blockDim_z, uint32_t blockIdx_x, uint32_t blockIdx_y, uint32_t blockIdx_z, uint32_t threadIdx_x, uint32_t threadIdx_y, uint32_t threadIdx_z) {\n  int64_t j;\n  int64_t i;\n  double stddev_root;\n\n// INSERT COMMENT IFELSE: kernel_stddev::entry\n  j = blockDim_x * blockIdx_x + threadIdx_x;\n  if (j < m) { // IFELSE MARKER: entry IF\n  stddev[j] = 0;\nfor(int64_t i = 0; i < n;   i = i + 1){\n  stddev[j] = (stddev[j] + ((data[(i * m + j)] - mean[j]) * (data[(i * m + j)] - mean[j])));\n}\n  stddev[j] = (stddev[j] / (double)(n));\n  stddev_root = sqrt(stddev[j]);\n  stddev[j] = stddev_root;\n  if (llvm_fcmp_ole(stddev[j], 0.10000000000000001)) { // IFELSE MARKER: for.end IF\n  stddev[j] = 1;\n  }\n  }\n  return;\n}\n// INSERT COMMENT FUNCTION: kernel_reduce\nvoid kernel_reduce(uint32_t m, uint32_t n, double* data, double* corr, double* mean, double* stddev, uint32_t gridDim_x, uint32_t gridDim_y, uint32_t gridDim_z, uint32_t blockDim_x, uint32_t blockDim_y, uint32_t blockDim_z, uint32_t blockIdx_x, uint32_t blockIdx_y, uint32_t blockIdx_z, uint32_t threadIdx_x, uint32_t threadIdx_y, uint32_t threadIdx_z) {\n  int32_t i;\n  int64_t j;\n  double sqrt_n;\n\n// INSERT COMMENT IFELSE: kernel_reduce::entry\n  i = blockDim_x * blockIdx_x + threadIdx_x;\n  j = blockDim_y * blockIdx_y + threadIdx_y;\n  if (i < n) { // IFELSE MARKER: entry IF\n  if (j < m) { // IFELSE MARKER: land.lhs.true IF\n  data[(i * m + j)] = (data[(i * m + j)] - mean[j]);\n  sqrt_n = sqrt((double)(n));\n  data[(i * m + j)] = (data[(i * m + j)] / (sqrt_n * stddev[j]));\n  }\n  }\n  return;\n}\n// INSERT COMMENT FUNCTION: kernel_diag\nvoid kernel_diag(uint32_t m, uint32_t n, double* data, double* corr, double* mean, double* stddev, uint32_t gridDim_x, uint32_t gridDim_y, uint32_t gridDim_z, uint32_t blockDim_x, uint32_t blockDim_y, uint32_t blockDim_z, uint32_t blockIdx_x, uint32_t blockIdx_y, uint32_t blockIdx_z, uint32_t threadIdx_x, uint32_t threadIdx_y, uint32_t threadIdx_z) {\n  int32_t i;\n\n// INSERT COMMENT IFELSE: kernel_diag::entry\n  i = blockDim_x * blockIdx_x + threadIdx_x;\n  if (i < m) { // IFELSE MARKER: entry IF\n  corr[(i * m + i)] = 1;\n  }\n  return;\n}\n// INSERT COMMENT FUNCTION: kernel_corr\nvoid kernel_corr(uint32_t m, uint32_t n, double* data, double* corr, double* mean, double* stddev, uint32_t gridDim_x, uint32_t gridDim_y, uint32_t gridDim_z, uint32_t blockDim_x, uint32_t blockDim_y, uint32_t blockDim_z, uint32_t blockIdx_x, uint32_t blockIdx_y, uint32_t blockIdx_z, uint32_t threadIdx_x, uint32_t threadIdx_y, uint32_t threadIdx_z) {\n  int32_t i;\n  int32_t j;\n  int64_t k;\n\n// INSERT COMMENT IFELSE: kernel_corr::entry\n  i = blockDim_x * blockIdx_x + threadIdx_x;\n  __auto_type blockDim_y_times_blockIdx_y = blockDim_y * blockIdx_y;\n  __auto_type threadIdx_y_val = threadIdx_y;\n  __auto_type y_index_base = blockDim_y_times_blockIdx_y + threadIdx_y_val;\n  __auto_type j_calc = y_index_base + i + 1;\n  j = j_calc;\n  if (i < (m - 1)) { // IFELSE MARKER: entry IF\n  if (j < m) { // IFELSE MARKER: land.lhs.true IF\n  __auto_type idx_i_m_j = i * m + j;\n  __auto_type corr_ij_old = corr[idx_i_m_j];\n  corr[(i * m + j)] = 0;\nfor(int64_t k = 0; k < n;   k = k + 1){\n  __auto_type idx_i_m_j_loop = i * m + j;\n  __auto_type idx_k_m_i = k * m + i;\n  __auto_type idx_k_m_j = k * m + j;\n  __auto_type prod_data_ki_kj = data[idx_k_m_i] * data[idx_k_m_j];\n  __auto_type corr_accum = corr[idx_i_m_j_loop] + prod_data_ki_kj;\n  corr[(i * m + j)] = corr_accum;\n}\n  __auto_type idx_j_m_i = j * m + i;\n  __auto_type idx_i_m_j_sym_src = i * m + j;\n  __auto_type corr_src_val = corr[idx_i_m_j_sym_src];\n  corr[(j * m + i)] = corr_src_val;\n  }\n  }\n  return;\n}\n// INSERT COMMENT FUNCTION: kernel_tail\nvoid kernel_tail(uint32_t m, uint32_t n, double* data, double* corr, double* mean, double* stddev, uint32_t gridDim_x, uint32_t gridDim_y, uint32_t gridDim_z, uint32_t blockDim_x, uint32_t blockDim_y, uint32_t blockDim_z, uint32_t blockIdx_x, uint32_t blockIdx_y, uint32_t blockIdx_z, uint32_t threadIdx_x, uint32_t threadIdx_y, uint32_t threadIdx_z) {\n  corr[(((m - 1) * m + m) - 1)] = 1;\n}\n// INSERT COMMENT FUNCTION: kernel\nvoid kernel(uint32_t m, uint32_t n, double* data, double* corr, double* mean, double* stddev) {\n  struct Dim3 grid_mean_blocks;    /* Address-exposed local */\n  struct Dim3 block_256x1;    /* Address-exposed local */\n  struct Dim3Coerce grid_mean_blocks_coerce;    /* Address-exposed local */\n  struct Dim3Coerce block_256x1_coerce;    /* Address-exposed local */\n  struct Dim3 grid_stddev_blocks;    /* Address-exposed local */\n  struct Dim3 block_256x1_stddev;    /* Address-exposed local */\n  struct Dim3Coerce grid_stddev_blocks_coerce;    /* Address-exposed local */\n  struct Dim3Coerce block_256x1_stddev_coerce;    /* Address-exposed local */\n  struct Dim3 block;    /* Address-exposed local */\n  struct Dim3 grid;    /* Address-exposed local */\n  struct Dim3 block_8_32;    /* Address-exposed local */\n  struct Dim3 grid_for_reduce;    /* Address-exposed local */\n  struct Dim3Coerce block_8_32_coerce;    /* Address-exposed local */\n  struct Dim3Coerce grid_for_reduce_coerce;    /* Address-exposed local */\n  struct Dim3 block_256x1_diag;    /* Address-exposed local */\n  struct Dim3 grid_diag_blocks;    /* Address-exposed local */\n  struct Dim3Coerce block_256x1_diag_coerce;    /* Address-exposed local */\n  struct Dim3Coerce grid_diag_blocks_coerce;    /* Address-exposed local */\n  struct Dim3 block_8_32_corr;    /* Address-exposed local */\n  struct Dim3 grid_for_corr;    /* Address-exposed local */\n  struct Dim3Coerce block_8_32_corr_coerce;    /* Address-exposed local */\n  struct Dim3Coerce grid_for_corr_coerce;    /* Address-exposed local */\n  struct Dim3 block_tail;    /* Address-exposed local */\n  struct Dim3 grid_tail;    /* Address-exposed local */\n  struct Dim3Coerce block_tail_coerce;    /* Address-exposed local */\n  struct Dim3Coerce grid_tail_coerce;    /* Address-exposed local */\n  int32_t num_blocks_m_256;\n  uint8_t* tmp_u8_1;\n  uint8_t* tmp_u8_2;\n  uint32_t i;\n  uint32_t j;\n  int32_t num_blocks_m_256_2;\n  uint8_t* tmp_u8_3;\n  uint8_t* tmp_u8_4;\n  int32_t num_blocks_n_batch8;\n  int32_t num_blocks_m_x32;\n  uint8_t* tmp_u8_5;\n  uint8_t* tmp_u8_6;\n  uint8_t* tmp_u8_7;\n  uint8_t* tmp_u8_8;\n  uint32_t k;\n  uint32_t l;\n  int32_t num_blocks_m_256_diag;\n  uint8_t* tmp_u8_9;\n  uint8_t* tmp_u8_10;\n  int32_t num_blocks_m1_batch8;\n  int32_t num_blocks_m1_x32;\n  uint8_t* tmp_u8_11;\n  uint8_t* tmp_u8_12;\n  uint8_t* tmp_u8_13;\n  uint8_t* tmp_u8_14;\n  uint8_t* tmp_u8_15;\n  uint8_t* tmp_u8_16;\n\n  num_blocks_m_256 = num_blocks(m, 256);\n  grid_mean_blocks.batch = num_blocks_m_256;\n  grid_mean_blocks.x = 1;\n  grid_mean_blocks.y = 1;\n  block_256x1.batch = 256;\n  block_256x1.x = 1;\n  block_256x1.y = 1;\n  memcpy(((uint8_t*)(&grid_mean_blocks_coerce)), ((uint8_t*)(&grid_mean_blocks)), 12);\n  memcpy(((uint8_t*)(&block_256x1_coerce)), ((uint8_t*)(&block_256x1)), 12);\n// INSERT COMMENT LOOP: kernel::header.0\n#pragma omp parallel for collapse(2)\nfor(int32_t i = 0; i < num_blocks_m_256;   i = i + 1){\nfor(int32_t j = 0; j < 256;   j = j + 1){\nkernel_mean(m, n, data, corr, mean, stddev, num_blocks_m_256, 1, 1, 256, 1, 1, i, 0, 0, j, 0, 0);\n}\n}\n  num_blocks_m_256_2 = num_blocks(m, 256);\n  grid_stddev_blocks.batch = num_blocks_m_256_2;\n  grid_stddev_blocks.x = 1;\n  grid_stddev_blocks.y = 1;\n  block_256x1_stddev.batch = 256;\n  block_256x1_stddev.x = 1;\n  block_256x1_stddev.y = 1;\n  memcpy(((uint8_t*)(&grid_stddev_blocks_coerce)), ((uint8_t*)(&grid_stddev_blocks)), 12);\n  memcpy(((uint8_t*)(&block_256x1_stddev_coerce)), ((uint8_t*)(&block_256x1_stddev)), 12);\n// INSERT COMMENT LOOP: kernel::header.034\n#pragma omp parallel for collapse(2)\nfor(int32_t i = 0; i < num_blocks_m_256_2;   i = i + 1){\nfor(int32_t j = 0; j < 256;   j = j + 1){\nkernel_stddev(m, n, data, corr, mean, stddev, num_blocks_m_256_2, 1, 1, 256, 1, 1, i, 0, 0, j, 0, 0);\n}\n}\n  block.batch = 8;\n  block.x = 32;\n  block.y = 1;\n  num_blocks_n_batch8 = num_blocks(n, block.batch);\n  num_blocks_m_x32 = num_blocks(m, block.x);\n  grid.batch = num_blocks_n_batch8;\n  grid.x = num_blocks_m_x32;\n  grid.y = 1;\n  memcpy(((uint8_t*)(&block_8_32)), ((uint8_t*)(&block)), 12);\n  memcpy(((uint8_t*)(&grid_for_reduce)), ((uint8_t*)(&grid)), 12);\n  memcpy(((uint8_t*)(&block_8_32_coerce)), ((uint8_t*)(&block_8_32)), 12);\n  memcpy(((uint8_t*)(&grid_for_reduce_coerce)), ((uint8_t*)(&grid_for_reduce)), 12);\n// INSERT COMMENT LOOP: kernel::header.044\n#pragma omp parallel for collapse(2)\nfor(int32_t i = 0; i < 8;   i = i + 1){\nfor(int32_t j = 0; j < 32;   j = j + 1){\nfor(int32_t k = 0; k < num_blocks_n_batch8;   k = k + 1){\nfor(int32_t l = 0; l < num_blocks_m_x32;   l = l + 1){\nkernel_reduce(m, n, data, corr, mean, stddev, 8, 32, 1, num_blocks_n_batch8, num_blocks_m_x32, 1, i, j, 0, k, l, 0);\n}\n}\n}\n}\n  block_256x1_diag.batch = 256;\n  block_256x1_diag.x = 1;\n  block_256x1_diag.y = 1;\n  num_blocks_m_256_diag = num_blocks(m, 256);\n  grid_diag_blocks.batch = num_blocks_m_256_diag;\n  grid_diag_blocks.x = 1;\n  grid_diag_blocks.y = 1;\n  memcpy(((uint8_t*)(&block_256x1_diag_coerce)), ((uint8_t*)(&block_256x1_diag)), 12);\n  memcpy(((uint8_t*)(&grid_diag_blocks_coerce)), ((uint8_t*)(&grid_diag_blocks)), 12);\n// INSERT COMMENT LOOP: kernel::header.054\n#pragma omp parallel for collapse(2)\nfor(int32_t i = 0; i < 256;   i = i + 1){\nfor(int32_t j = 0; j < num_blocks_m_256_diag;   j = j + 1){\nkernel_diag(m, n, data, corr, mean, stddev, 256, 1, 1, num_blocks_m_256_diag, 1, 1, i, 0, 0, j, 0, 0);\n}\n}\n  block.batch = 8;\n  block.x = 32;\n  block.y = 1;\n  num_blocks_m1_batch8 = num_blocks((m - 1), block.batch);\n  num_blocks_m1_x32 = num_blocks((m - 1), block.x);\n  grid.batch = num_blocks_m1_batch8;\n  grid.x = num_blocks_m1_x32;\n  grid.y = 1;\n  memcpy(((uint8_t*)(&block_8_32_corr)), ((uint8_t*)(&block)), 12);\n  memcpy(((uint8_t*)(&grid_for_corr)), ((uint8_t*)(&grid)), 12);\n  memcpy(((uint8_t*)(&block_8_32_corr_coerce)), ((uint8_t*)(&block_8_32_corr)), 12);\n  memcpy(((uint8_t*)(&grid_for_corr_coerce)), ((uint8_t*)(&grid_for_corr)), 12);\n// INSERT COMMENT LOOP: kernel::header.064\n#pragma omp parallel for collapse(2)\nfor(int32_t i = 0; i < 8;   i = i + 1){\nfor(int32_t j = 0; j < 32;   j = j + 1){\nfor(int32_t k = 0; k < num_blocks_m1_batch8;   k = k + 1){\nfor(int32_t l = 0; l < num_blocks_m1_x32;   l = l + 1){\nkernel_corr(m, n, data, corr, mean, stddev, 8, 32, 1, num_blocks_m1_batch8, num_blocks_m1_x32, 1, i, j, 0, k, l, 0);\n}\n}\n}\n}\n// INSERT COMMENT IFELSE: kernel::kcall.end37\n  block_tail.batch = 1;\n  block_tail.x = 1;\n  block_tail.y = 1;\n  grid_tail.batch = 1;\n  grid_tail.x = 1;\n  grid_tail.y = 1;\n  memcpy(((uint8_t*)(&block_tail_coerce)), ((uint8_t*)(&block_tail)), 12);\n  memcpy(((uint8_t*)(&grid_tail_coerce)), ((uint8_t*)(&grid_tail)), 12);\n  ;\n  if (0) { // IFELSE MARKER: kcall.end37 IF\n  } else { // IFELSE MARKER: kcall.end37 ELSE\nkernel_tail(m, n, data, corr, mean, stddev, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0);\n  }\n  return;\n}\n\n```"
}